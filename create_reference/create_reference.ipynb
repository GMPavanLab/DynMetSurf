{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load References\n",
    "\n",
    "This notebook will create the reference to be used in the top-down analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5py import File\n",
    "from HDF5er import saveXYZfromTrajGroup,MDA2HDF5,saveXYZfromTrajGroup\n",
    "import numpy\n",
    "from MDAnalysis import Universe as mdaUniverse\n",
    "from SOAPify import (saponifyGroup, \n",
    "                    createReferencesFromTrajectory,\n",
    "                    mergeReferences,\n",
    "                    SOAPdistanceNormalized,\n",
    "                    saveReferences,\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadReferences=True\n",
    "soapReferences=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if loadReferences:\n",
    "    for fname in [\"110.data\"  ,\"111.data\"  ,\"210.data\"  ,\"211.data\" ]:\n",
    "        u=mdaUniverse(fname)#, atom_style=\"id type x y z\")\n",
    "        u.atoms.types = [\"Cu\"] * len(u.atoms)\n",
    "        print(u.coord[0])\n",
    "        MDA2HDF5(u,\"bases.hdf5\",fname.split('.')[0], trajChunkSize=1000)\n",
    "\n",
    "    with File(\"bases.hdf5\",\"r\") as workFile:\n",
    "        for id in ['111','211','210','110']:\n",
    "            saveXYZfromTrajGroup(f\"{id}.xyz\",workFile[f'Trajectories/{id}'])\n",
    "if soapReferences:\n",
    "    with File(\"bases.hdf5\",\"a\") as workFile:\n",
    "        saponifyGroup(\n",
    "        trajContainers=workFile[\"Trajectories\"],\n",
    "        SOAPoutContainers=workFile.require_group(\"SOAP\"),\n",
    "        SOAPOutputChunkDim=1000,\n",
    "        SOAPnJobs=32,\n",
    "        SOAPrcut=6,    \n",
    "        SOAPnmax= 8,\n",
    "        SOAPlmax= 8,\n",
    "    )\n",
    "\n",
    "references={}\n",
    "request={\n",
    "    \"111\":dict(s=(0,1312),ss=(0,1313),b=(0,1099)),\n",
    "    \"110\":dict(slc=(0,1072),shc=(0,1089),sslc=(0,1074),sshc=(0,1091),b=(0,1080)),\n",
    "    \"211\":dict(slc=(0,1176),smc=(0,1297),shc=(0,1202),sslc=(0,1275),ssmc=(0,1204),sshc=(0,1301),b=(0,1309)),\n",
    "    \"210\":dict(slc=(0,1320),smc=(0,1297),shc=(0,1298),sslc=(0,1611),ssmc=(0,1324),sshc=(0,1301),b=(0,1308))\n",
    "}\n",
    "with File(\"bases.hdf5\",\"r\") as workFile:\n",
    "    for k in request:\n",
    "        references[k]=createReferencesFromTrajectory(workFile[f'SOAP/{k}'],request[k],8,8)\n",
    "        for i,name in enumerate(references[k].names):\n",
    "            references[k].names[i]=f'{k}_{name}'\n",
    "    \n",
    "\n",
    "wholeData=mergeReferences(references['111'],references['110'],references['211'],references['210'])\n",
    "ndataset=len(wholeData)\n",
    "wholeDistances=numpy.zeros((int(ndataset*(ndataset-1)/2)))\n",
    "cpos=0\n",
    "for i in range(ndataset):\n",
    "    for j in range(i+1,ndataset):\n",
    "        wholeDistances[cpos]=SOAPdistanceNormalized(wholeData.spectra[i],wholeData.spectra[j])\n",
    "        cpos+=1\n",
    "\n",
    "\n",
    "with File(\"references.hdf5\",'w') as refFile:\n",
    "    g=refFile.require_group('testReferences')\n",
    "    for k in references:\n",
    "        saveReferences(g,k,references[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "23681d40f44fdf1a19dbb39f5bce5b718db086fdf50cc84aac2f136406f4c027"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
