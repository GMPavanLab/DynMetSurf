{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plp\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from  h5py import File\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import h5py\n",
    "from HDF5er import saveXYZfromTrajGroup,MDA2HDF5,saveXYZfromTrajGroup\n",
    "import numpy\n",
    "from MDAnalysis import Universe as mdaUniverse\n",
    "from SOAPify import (saponifyGroup,\n",
    "                    createReferencesFromTrajectory,\n",
    "                    mergeReferences,\n",
    "                    SOAPdistanceNormalized,\n",
    "                    saveReferences,\n",
    "                    getReferencesFromDataset,\n",
    "                    classify\n",
    "                    )\n",
    "\n",
    "loadReferences=True\n",
    "soapReferences=True\n",
    "\n",
    "\n",
    "\n",
    "def patchBoxFromTopology(hdf5TrajFile:str,topologyFile:str):\n",
    "    u=mdaUniverse(topologyFile,atom_style=\"id type x y z\")\n",
    "    with h5py.File(hdf5TrajFile,\"a\") as workFile:\n",
    "        for key in workFile['Trajectories']:\n",
    "            tgroup=workFile[f'Trajectories/{key}']\n",
    "            tgroup['Box'][:]=[u.dimensions]*tgroup['Box'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCompactedAnnotationsForTmat(tmat) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of compacted annotations for a given tmat.\n",
    "    \"\"\"\n",
    "    annot = list(numpy.empty(tmat.shape, dtype=str))\n",
    "    # annot=numpy.chararray(tmat.shape, itemsize=5)\n",
    "    for row in range(tmat.shape[0]):\n",
    "        annot[row] = list(annot[row])\n",
    "        for col in range(tmat.shape[1]):\n",
    "            if tmat[row, col] < 0.01:\n",
    "                annot[row][col] = f\"<0.01\"\n",
    "            elif tmat[row, col] > 0.99:\n",
    "                annot[row][col] = f\">0.99\"\n",
    "            else:\n",
    "                annot[row][col] = f\"{tmat[row,col]:.2f}\"\n",
    "    return annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertClusteringV(classification:numpy.ndarray, conversion:numpy.ndarray):\n",
    "    myf=lambda x: conversion[x]\n",
    "    vfunc = numpy.vectorize(myf)\n",
    "    t=vfunc(classification.flatten())\n",
    "    \n",
    "\n",
    "    return t.reshape(classification.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we evaluate the distances between the soap environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = {}\n",
    "with File(\"../../create_reference/references.hdf5\", \"r\") as refFile:\n",
    "    g = refFile[\"testReferences\"]\n",
    "    for k in g.keys():\n",
    "        references[k] = getReferencesFromDataset(g[k])\n",
    "\n",
    "wholeData = mergeReferences(\n",
    "    references[\"111\"], references[\"110\"], references[\"211\"], references[\"210\"])\n",
    "\n",
    "ref1=wholeData\n",
    "ndataset = len(ref1)\n",
    "r1 = numpy.zeros((int(ndataset * (ndataset - 1) / 2)))\n",
    "cpos = 0\n",
    "for i in range(ndataset):\n",
    "    for j in range(i + 1, ndataset):\n",
    "        r1[cpos] = SOAPdistanceNormalized(\n",
    "            ref1.spectra[i], ref1.spectra[j]\n",
    "        )\n",
    "        cpos += 1\n",
    "        \n",
    "links=sch.linkage(r1,method=\"complete\")\n",
    "clusterCut={}\n",
    "for cut in [0.01]: \n",
    "    c=sch.fcluster(links,t=cut, criterion=\"distance\")\n",
    "    clusterCut[str(cut).replace(\".\",\"_\")]=c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we obtain the hierarchy dendrogram cut at 0.01 d_soap; This allowes us to  merge very similar SOAP environments into\n",
    "common macro-clusters: e.g., bulk (b*), sub-surface (ss*), surface (s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut=0.01\n",
    "import matplotlib.pyplot as plt\n",
    "links=sch.linkage(r1,method=\"complete\")\n",
    "fig,ax=plp.subplots(1,1, dpi=150)\n",
    "plt.rcParams[\"figure.figsize\"] = [5.50, 5.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "dendro=sch.dendrogram(links,color_threshold=cut,labels=wholeData.names, orientation=\"left\", \n",
    "                      ax=ax,above_threshold_color='black')\n",
    "clusters=sch.fcluster(links,t=cut, criterion=\"distance\")\n",
    "Name_dict=dict()\n",
    "names=[\"\" for i in range (0,max(clusters))]\n",
    "for i,n in enumerate(wholeData.names):\n",
    "    print(i,n,clusters[i])\n",
    "    Name_dict[n]=clusters[i]-1\n",
    "    names[clusters[i]-1]+=' ' + n\n",
    "names    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we classify trajectory using the atomic environments defined in the complete dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../211.hdf5', \"r\") as workFile:\n",
    "        g=workFile[f\"SOAP\"]\n",
    "        for key in workFile[f\"SOAP\"].keys():\n",
    "            cls = {}\n",
    "            t= classify(g[key], wholeData, SOAPdistanceNormalized, True)\n",
    "            cls[f\"whole\"] = t.references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert the classification derived from the complete dictionary, to that obtained with the cut at 0.01 d_soap.\n",
    "we then save the trajectory with the new classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../211.hdf5', \"r\") as workFile:\n",
    "        for key in workFile[f\"SOAP\"].keys():\n",
    "\n",
    "            for k in clusterCut:\n",
    "                cls[k] = convertClusteringV(cls[f\"whole\"],clusterCut[k]) \n",
    "            saveXYZfromTrajGroup(\n",
    "                f\"211_T_700_001.xyz\",\n",
    "                workFile[f\"Trajectories/{key}\"],\n",
    "                **cls,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we selected only the atoms belonging to the most top layers to calculate the transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../211.hdf5', \"r\") as workFile:\n",
    "    mask=workFile[\"Trajectories/211_T_700/Trajectory\"][0,:,2]>5.0\n",
    "    x=cls[k][:][:,mask]\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assign each cluster to the ideal surfaces (0=111,1=110,2=211,3=210), in order to obtain the transition matrix between native and non-native environments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clusters_cut_211 = {\n",
    "     '1' : 0,\n",
    "     '2' : 2,\n",
    "     '3' : 3,\n",
    "     '4' : 1,\n",
    "     '5' : 3,\n",
    "     '6' : 2,\n",
    "    '7' :  1,\n",
    "    '8' :  2,\n",
    "    '9' :  2,\n",
    "    '10':  0,\n",
    "    '11':  3,\n",
    "    '12':  2,\n",
    "    '13':  2,\n",
    "    '14':  1,\n",
    "    '15':  2,\n",
    "    '16':  3,}     \n",
    "new_array = numpy.empty(numpy.shape(x),dtype=numpy.int64)\n",
    "for row_idx, row in enumerate(x):\n",
    "    for col_idx , col in enumerate(row):\n",
    "        new_array[row_idx,col_idx] = new_clusters_cut_211[str(col)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from SOAPify import (SOAPclassification,\n",
    "transitionMatrixFromSOAPClassificationNormalized as TransitionMatrixMaker,\n",
    "transitionMatrixFromSOAPClassification as TransitionMatrixMakerNotNorm,\n",
    ")\n",
    "#names=[]\n",
    "names_4=['111','110','211','210']\n",
    "plt.rcParams.update({'font.size': 35})\n",
    "classification = SOAPclassification(\n",
    "None, \n",
    "new_array[250:],\n",
    "names_4[:]\n",
    "#names[:],\n",
    ")\n",
    "matrix_cl=TransitionMatrixMaker(classification, 1)\n",
    "#annot=getCompactedAnnotationsForTmat(matrix_cl)\n",
    "mask=matrix_cl==0\n",
    "fig, ax = plt.subplots(figsize=(10,10)) \n",
    "matrix_name = DataFrame(matrix_cl,index=names_4,columns=names_4)\n",
    "ax = sns.heatmap(matrix_name*100,linewidths=0.1,ax=ax, fmt=\".0f\", annot=True,square=True,cmap=\"rocket_r\", \n",
    "                 vmax=100, vmin=0, \n",
    "                 mask=mask,cbar=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "production",
   "language": "python",
   "name": "production"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
