{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80488e3b",
   "metadata": {},
   "source": [
    "# SOAP analysis\n",
    "\n",
    "In this notebook we present how we calculated the soap fingerprints\n",
    "Before executing this notebook, please start and complete the notebook `../create_reference/create_reference.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "from  h5py import File\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy\n",
    "from MDAnalysis import Universe as mdaUniverse\n",
    "from matplotlib.pyplot import viridis, get_cmap\n",
    "import seaborn as sns\n",
    "from seaborn import clustermap\n",
    "from HDF5er import saveXYZfromTrajGroup,MDA2HDF5,saveXYZfromTrajGroup\n",
    "from SOAPify import (saponifyGroup,\n",
    "                    createReferencesFromTrajectory,\n",
    "                    mergeReferences,\n",
    "                    SOAPdistanceNormalized,\n",
    "                    saveReferences,\n",
    "                    getReferencesFromDataset,\n",
    "                    classify\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "def patchBoxFromTopology(hdf5TrajFile:str,topologyFile:str):\n",
    "    u=mdaUniverse(topologyFile,atom_style=\"id type x y z\")\n",
    "    with File(hdf5TrajFile,\"a\") as workFile:\n",
    "        for key in workFile['Trajectories']:\n",
    "            tgroup=workFile[f'Trajectories/{key}']\n",
    "            tgroup['Box'][:]=[u.dimensions]*tgroup['Box'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6066d0",
   "metadata": {},
   "source": [
    "### Settings\n",
    "\n",
    "Please change `SOAPnJobs` to a reasonable number to not stress too much your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efee7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadReferences = True\n",
    "soapReferences = True\n",
    "SOAPnJobs = 16\n",
    "SOAPrcut = 6\n",
    "SOAPnmax = 8\n",
    "SOAPlmax = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing the trajectories and calcolating the SOAP fingerprints \n",
    "Here we preprocess the trajectories in the hdf5 files and we calculate the SOAP fingerprints by usin dscribe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loadReferences:\n",
    "    Temp=700\n",
    "    for surf in [110,211,210]:\n",
    "        for fname in [f\"{surf}_T_{Temp}.lammpsdump\"]:\n",
    "            dirname=f\"../DPMD/{surf}/{Temp}/\"\n",
    "            u=mdaUniverse(dirname+fname)#, atom_style=\"id type x y z\")\n",
    "            u.atoms.types = [\"Cu\"] * len(u.atoms)\n",
    "            MDA2HDF5(u,f\"{surf}.hdf5\",fname.split('.')[0], trajChunkSize=1000)\n",
    "            # we need to patch the box because mda do not load correctpy the triclinic box,\n",
    "            # at the moment of writing this the issue https://github.com/MDAnalysis/mdanalysis/issues/3383 has been addressed,\n",
    "            # but if you are using an old vesrion of MDA you will still encounter the problem\n",
    "            patchBoxFromTopology(f\"{surf}.hdf5\",f\"{surf}.data\")\n",
    "            print(fname, \"done\")\n",
    "\n",
    "    loadReferences=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3751e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if soapReferences:\n",
    "    for surf in [110,211,210]:\n",
    "        \n",
    "        with File(f\"{surf}.hdf5\",\"a\") as workFile:\n",
    "            saponifyGroup(\n",
    "            trajContainers=workFile[\"Trajectories\"],\n",
    "            SOAPoutContainers=workFile.require_group(\"SOAP\"),\n",
    "            SOAPOutputChunkDim=1000,\n",
    "            SOAPnJobs=SOAPnJobs,\n",
    "            SOAPrcut = SOAPrcut,\n",
    "            SOAPnmax = SOAPnmax,\n",
    "            SOAPlmax = SOAPlmax,\n",
    "        )\n",
    "    soapReferences=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the dictionary\n",
    "Here we load the dictionary whose component have been preprocessed in `../create_reference/create_reference.ipynb`.\n",
    "We assign a neam to each memeber, knowing their position\n",
    "We then caclulate the distance between each dictionary entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = {}\n",
    "with File(\"../create_reference/references.hdf5\", \"r\") as refFile:\n",
    "    g = refFile[\"testReferences\"]\n",
    "    for k in g.keys():\n",
    "        references[k] = getReferencesFromDataset(g[k])\n",
    "\n",
    "wholeData = mergeReferences(\n",
    "    references[\"111\"],\n",
    "    references[\"110\"],\n",
    "    references[\"211\"],\n",
    "    references[\"210\"],\n",
    ")\n",
    "dictionaryEntries = ['s$_{(111)}$','ss$_{(111)}$','b$_{(111)}$']+\\\n",
    "    ['s$^{l}$$_{(110)}$','s$^{h}$$_{(110)}$','ss$^{l}$$_{(110)}$','ss$^{h}$$_{(110)}$','b$_{(110)}$']+\\\n",
    "    ['s$^{l}$$_{(211)}$','s$^{m}$$_{(211)}$','s$^{h}$$_{(211)}$','ss$^{l}$$_{(211)}$','ss$^{m}$$_{(211)}$','ss$^{h}$$_{(211)}$','b$_{(211)}$']+\\\n",
    "    ['s$^{l}$$_{(210)}$','s$^{m}$$_{(210)}$','s$^{h}$$_{(210)}$','ss$^{l}$$_{(210)}$','ss$^{m}$$_{(210)}$','ss$^{h}$$_{(210)}$','b$_{(210)}$']   \n",
    "\n",
    "ndataset = len(wholeData)\n",
    "#distance calculated in the squareform fashion:\n",
    "wholeDistances = numpy.zeros((int(ndataset * (ndataset - 1) / 2)))\n",
    "cpos = 0\n",
    "for i in range(ndataset):\n",
    "    for j in range(i + 1, ndataset):\n",
    "        wholeDistances[cpos] = SOAPdistanceNormalized(\n",
    "            wholeData.spectra[i], wholeData.spectra[j]\n",
    "        )\n",
    "        cpos += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eafaa8a",
   "metadata": {},
   "source": [
    "We then assign a color to each element of the dictionary and we create a dendrogram from it.\n",
    "In the dendrogram the colored branches are the ones whose elemnets are at less than 0.01 \\[SOAPdist\\] between each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9590768",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {}\n",
    "colorbytype = {}\n",
    "cmaps = {\n",
    "    \"Oranges\": get_cmap(\"Oranges\"),\n",
    "    \"Reds\": get_cmap(\"Reds\"),\n",
    "    \"Blues\": get_cmap(\"Blues\"),\n",
    "    \"Greens\": get_cmap(\"Greens\"),\n",
    "    \"GnBu\": get_cmap(\"GnBu\"),\n",
    "    \"Greys\": get_cmap(\"Greys\"),\n",
    "    \"Purples\": get_cmap(\"Purples\"),\n",
    "}\n",
    "for k, c in zip(references.keys(), [\"Greys\", \"Reds\", \"GnBu\", \"Purples\"]):\n",
    "    colors[k] = [cmaps[c](0.6) for i in range(len(references[k]))]\n",
    "    colorbytype[k] = []\n",
    "    for i in range(len(references[k])):\n",
    "        cc = \"Oranges\"\n",
    "        num = 0.6\n",
    "        if \"ss\" in references[k].names[i]:\n",
    "            cc = \"Greens\"\n",
    "        elif \"s\" in references[k].names[i]:\n",
    "            cc = \"Blues\"\n",
    "        \n",
    "        if \"lc\" in references[k].names[i]:\n",
    "            num = 0.4\n",
    "        elif \"hc\" in references[k].names[i]:\n",
    "            num = 0.8\n",
    "        colorbytype[k].append(cmaps[cc](num))\n",
    "\n",
    "cr = [] + colors[\"111\"] + colors[\"110\"] + colors[\"211\"] + colors[\"210\"]\n",
    "cc = (\n",
    "    []\n",
    "    + colorbytype[\"111\"]\n",
    "    + colorbytype[\"110\"]\n",
    "    + colorbytype[\"211\"]\n",
    "    + colorbytype[\"210\"]\n",
    ")\n",
    "\n",
    "links = linkage(wholeDistances, method=\"complete\")\n",
    "fig=plt.figure(dpi = 200)\n",
    "ax=fig.add_subplot(111)\n",
    "sch.dendrogram(links, color_threshold=0.01,labels=dictionaryEntries,ax=ax,above_threshold_color='k')\n",
    "sns.despine(ax=ax,left=True,bottom=True)\n",
    "ax.get_yaxis().set_visible(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431e981",
   "metadata": {},
   "source": [
    "We display the distances in a heatmap, and we order the dictionary elements by the dendorgam that we calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 32})\n",
    "cmap = clustermap(\n",
    "    DataFrame(squareform(wholeDistances)),\n",
    "    method=\"complete\",\n",
    "    cmap=\"bone\",\n",
    "    tree_kws=dict(linewidths=2.5),\n",
    "    #row_colors=cr,\n",
    "    #col_colors=cc,\n",
    "    row_linkage=links,\n",
    "    col_linkage=links,\n",
    "    #linewidths=0.05,\n",
    "   dendrogram_ratio=(0.2,0.2),\n",
    "   linewidths=.75, \n",
    "    figsize=(25,25),\n",
    "    xticklabels=dictionaryEntries, yticklabels=dictionaryEntries,\n",
    "    cbar=True,\n",
    ")\n",
    "\n",
    "cmap.ax_col_dendrogram.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the dictionary to the trajectories\n",
    "\n",
    "Finally we apply the dictionary to our trajectories, and we export the result to an xyz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for surf in [110,211,210]:\n",
    "    with File(f\"{surf}.hdf5\", \"r\") as workFile:\n",
    "        g=workFile[f\"SOAP\"]\n",
    "        for key in workFile[f\"SOAP\"].keys():\n",
    "            cls = {}\n",
    "            t= classify(g[key], wholeData, SOAPdistanceNormalized, True)\n",
    "            cls[f\"whole\"] = t.references\n",
    "            cls[f\"whole_d\"] = t.distances\n",
    "            saveXYZfromTrajGroup(\n",
    "             f\"whole_{surf}_T_700.xyz\",\n",
    "             workFile[f\"Trajectories/{key}\"],\n",
    "             **cls,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "23681d40f44fdf1a19dbb39f5bce5b718db086fdf50cc84aac2f136406f4c027"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
