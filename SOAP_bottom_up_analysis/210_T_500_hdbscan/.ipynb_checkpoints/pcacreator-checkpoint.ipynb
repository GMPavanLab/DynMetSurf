{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from sklearn.decomposition import PCA\n",
    "import SOAPify\n",
    "from sys import argv\n",
    "\n",
    "lmax=8\n",
    "nmax=8\n",
    "\n",
    "def preparePCAFitSet(fitsetGroup: h5py.Group, PCAdim: int):\n",
    "    print(\"preparing the PCA with from the fitset\")\n",
    "    fitset = fitsetGroup[\"SOAPFitSet\"][:]\n",
    "    fitset= SOAPify.fillSOAPVectorFromdscribe(fitset, lmax, nmax)\n",
    "    fitset = SOAPify.normalizeArray(fitset)\n",
    "    pcaMaker = PCA(PCAdim)\n",
    "    pcaMaker.fit(fitset[:])\n",
    "    return pcaMaker\n",
    "\n",
    "#if __name__ == \"__main__\": #capisce di lanciarlo come programma\n",
    "def applypca(fname):\n",
    "#uso come base per il fit i dati che sono nel file della variabile  pcaFilename \n",
    "    referencepcaFilename = \"fitsets_red.hdf5\"\n",
    "    pcaname = \"test210_redux\"\n",
    "    chunklen = 100\n",
    "    pcadim = 3\n",
    "    with h5py.File(referencepcaFilename, \"r\") as fsFile, h5py.File(fname,\"a\") as SOAPFile:\n",
    "        pcaEngine = preparePCAFitSet(fsFile[pcaname], pcadim)\n",
    "        pcaGroup = SOAPFile.require_group(f\"PCAs/{pcaname}\")\n",
    "        pcaGroup.attrs[\"PCAOrigin\"] = f\"{referencepcaFilename}/{pcaname}\"\n",
    "\n",
    "        for key in SOAPFile[\"SOAP\"].keys():\n",
    "\n",
    "            print(f\"appling PCA to {key}\")\n",
    "\n",
    "            data = SOAPFile[\"SOAP\"][key]\n",
    "            pcaout = pcaGroup.require_dataset(\n",
    "                key,\n",
    "                shape=(data.shape[0], data.shape[1], pcadim),\n",
    "                dtype=data.dtype,\n",
    "                chunks=(chunklen, data.shape[1], pcadim),\n",
    "                maxshape=(None, data.shape[1], pcadim),\n",
    "                compression=\"gzip\",\n",
    "            )\n",
    "            for chunkTraj in data.iter_chunks():\n",
    "                print(f'{key}:working on SOAP chunk \"{chunkTraj}\"')\n",
    "                normalizedData = SOAPify.normalizeArray(\n",
    "                    SOAPify.fillSOAPVectorFromdscribe(data[chunkTraj], lmax, nmax))\n",
    "                pcaRes = pcaEngine.transform(\n",
    "                    normalizedData.reshape((-1, normalizedData.shape[-1]))\n",
    "                )\n",
    "                pcaout[chunkTraj[0]] = pcaRes.reshape((-1, data.shape[1], pcadim))\n",
    "\n",
    "            pcaout.attrs[\"variance\"] = pcaEngine.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plp\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from  h5py import File\n",
    "from pandas import DataFrame\n",
    "import h5py\n",
    "from HDF5er import saveXYZfromTrajGroup,MDA2HDF5,saveXYZfromTrajGroup\n",
    "import numpy\n",
    "from MDAnalysis import Universe as mdaUniverse\n",
    "from SOAPify import (saponifyGroup,\n",
    "                    createReferencesFromTrajectory,\n",
    "                    mergeReferences,\n",
    "                    SOAPdistanceNormalized,\n",
    "                    saveReferences,\n",
    "                    getReferencesFromDataset,\n",
    "                    classify\n",
    "                    )\n",
    "\n",
    "loadReferences=True\n",
    "soapReferences=True\n",
    "\n",
    "\n",
    "\n",
    "def patchBoxFromTopology(hdf5TrajFile:str,topologyFile:str):\n",
    "    u=mdaUniverse(topologyFile,atom_style=\"id type x y z\")\n",
    "    with h5py.File(hdf5TrajFile,\"a\") as workFile:\n",
    "        for key in workFile['Trajectories']:\n",
    "            tgroup=workFile[f'Trajectories/{key}']\n",
    "            tgroup['Box'][:]=[u.dimensions]*tgroup['Box'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loadReferences:\n",
    "    for surf in [\"210\"]:\n",
    "        for fname in [f\"210_T_500.lammpsdump\" ]:\n",
    "            u=mdaUniverse(fname)#, atom_style=\"id type x y z\")\n",
    "            u.atoms.types = [\"Cu\"] * len(u.atoms)\n",
    "            print(u.coord[0])\n",
    "            MDA2HDF5(u,f\"{surf}.hdf5\",fname.split('.')[0], trajChunkSize=1000)\n",
    "            print(surf)\n",
    "if soapReferences:\n",
    "    for surf in [\"210\"]:\n",
    "        patchBoxFromTopology(f\"{surf}.hdf5\",f\"210.data\") \n",
    "        with File(f\"{surf}.hdf5\",\"a\") as workFile:\n",
    "            saponifyGroup(\n",
    "            trajContainers=workFile[\"Trajectories\"],\n",
    "            SOAPoutContainers=workFile.require_group(\"SOAP\"),\n",
    "            SOAPOutputChunkDim=1000,\n",
    "            SOAPnJobs=32,\n",
    "            SOAPrcut=6,\n",
    "            SOAPnmax= 8,\n",
    "            SOAPlmax= 8,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing the PCA with from the fitset\n",
      "appling PCA to 210_T_700\n",
      "210_T_700:working on SOAP chunk \"(slice(0, 100, 1), slice(0, 2304, 1), slice(0, 324, 1))\"\n",
      "210_T_700:working on SOAP chunk \"(slice(100, 200, 1), slice(0, 2304, 1), slice(0, 324, 1))\"\n",
      "210_T_700:working on SOAP chunk \"(slice(200, 300, 1), slice(0, 2304, 1), slice(0, 324, 1))\"\n",
      "210_T_700:working on SOAP chunk \"(slice(300, 400, 1), slice(0, 2304, 1), slice(0, 324, 1))\"\n",
      "210_T_700:working on SOAP chunk \"(slice(400, 500, 1), slice(0, 2304, 1), slice(0, 324, 1))\"\n",
      "210_T_700:working on SOAP chunk \"(slice(500, 502, 1), slice(0, 2304, 1), slice(0, 324, 1))\"\n"
     ]
    }
   ],
   "source": [
    "applypca(\"210.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "production",
   "language": "python",
   "name": "production"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
